{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83325977-9b05-41e7-ae89-17f03a5ad414",
   "metadata": {},
   "source": [
    "# function def and uploading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f057723-1cac-4167-a1fa-114ab1042ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441763ce-fd71-46da-995d-5e479d0520a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, column_to_check, columns_to_drop=None, train_size=0.7):\n",
    "    \"\"\"\n",
    "    Prepares the dataset by splitting it into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The dataset (DataFrame).\n",
    "    - column_to_check: The target variable (dependent variable).\n",
    "    - columns_to_drop: Optional; additional columns to drop before training.\n",
    "    - train_size: The proportion of data to use for training (default 70%).\n",
    "\n",
    "    Returns:\n",
    "    - X_train: Training features\n",
    "    - X_test: Testing features\n",
    "    - y_train: Training target values\n",
    "    - y_test: Testing target values\n",
    "    \"\"\"\n",
    "\n",
    "    drop = []\n",
    "    if columns_to_drop:\n",
    "        # Ensure columns_to_drop is a list\n",
    "        drop = [columns_to_drop] if isinstance(columns_to_drop, str) else columns_to_drop\n",
    "\n",
    "    # X: Features (drop specified columns + target column)\n",
    "    X = data.drop(drop + [column_to_check], axis=1)\n",
    "\n",
    "    # y: Target variable (dependent variable)\n",
    "    y = data[column_to_check]\n",
    "\n",
    "    # Splitting data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=42, train_size=train_size\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Function to calculate Root Mean Squared Error (RMSE)\n",
    "def RMSE(y_, y_pred_):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Squared Error (RMSE).\n",
    "\n",
    "    Parameters:\n",
    "    - y_: Actual values\n",
    "    - y_pred_: Predicted values\n",
    "\n",
    "    Returns:\n",
    "    - RMSE value\n",
    "    \"\"\"\n",
    "    return ((y_ - y_pred_) ** 2).mean() ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b53e7a-d3e7-4822-8065-e7bab6373cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload train and valid data sets\n",
    "df_train = pd.read_csv(file_path,\n",
    "                 low_memory=False,\n",
    "                 parse_dates=[\"saledate\"])\n",
    "\n",
    "df_valid = pd.read_csv(file_path,\n",
    "                 low_memory=False,\n",
    "                 parse_dates=[\"saledate\"])\n",
    "\n",
    "farm_data = pd.read_csv(file_path,\n",
    "                 low_memory=False)\n",
    "\n",
    "df_train.info()\n",
    "df_valid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a51006-cb83-41b1-a504-313898c1f7ba",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cceaba-4221-49c8-9aae-31f9c85c05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# ========================\n",
    "# Train preparation\n",
    "# ========================\n",
    "# ========================\n",
    "\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Sort saledate and copy data\n",
    "# ========================\n",
    "\n",
    "# Sort by saledate in ascending order\n",
    "df_train.sort_values(by=[\"saledate\"], inplace=True, ascending=True)\n",
    "\n",
    "# Copy dataset for preprocessing into df_tmp3\n",
    "df_tmp3 = df_train.copy()\n",
    "\n",
    "# ========================\n",
    "# Extract Date Features\n",
    "# ========================\n",
    "\n",
    "df_tmp3[\"saleYear\"]    = df_tmp3.saledate.dt.year\n",
    "df_tmp3[\"saleMonth\"]   = df_tmp3.saledate.dt.month\n",
    "df_tmp3[\"saleDay\"]     = df_tmp3.saledate.dt.day\n",
    "df_tmp3[\"saleDayofweek\"] = df_tmp3.saledate.dt.dayofweek\n",
    "df_tmp3[\"saleDayofyear\"] = df_tmp3.saledate.dt.dayofyear\n",
    "\n",
    "# Drop original saledate column\n",
    "df_tmp3.drop(\"saledate\", axis=1, inplace=True)\n",
    "\n",
    "# ========================\n",
    "# Transform object columns to categories\n",
    "# ========================\n",
    "\n",
    "# Transform object columns to category\n",
    "for col in df_tmp3.select_dtypes(['object']):\n",
    "    df_tmp3[col] = df_tmp3[col].astype('category')\n",
    "\n",
    "# ========================\n",
    "# filling nulls\n",
    "# ========================\n",
    "\n",
    "# Initialize an empty list to store column names that contain \"None or Unspecified\"\n",
    "columns_with_none_or_unspecified = []\n",
    "\n",
    "# Iterate over categorical columns\n",
    "for col in df_tmp3.select_dtypes(include=[\"category\", \"object\"]):\n",
    "    if df_tmp3[col].astype(str).str.contains(\"None or Unspecified\", na=False).any():\n",
    "        columns_with_none_or_unspecified.append(col)\n",
    "\n",
    "# Fill nulls in columns that contain \"None or Unspecified\"\n",
    "for col in columns_with_none_or_unspecified:\n",
    "    df_tmp3[col] = df_tmp3[col].fillna(\"None or Unspecified\")\n",
    "\n",
    "# Find categorical columns with null values\n",
    "categorical_columns_with_nulls = df_tmp3.select_dtypes(include=[\"category\", \"object\"]).isnull().sum()\n",
    "categorical_columns_with_nulls = categorical_columns_with_nulls[categorical_columns_with_nulls > 0]\n",
    "\n",
    "# Convert to a list\n",
    "categorical_columns_with_nulls_list = categorical_columns_with_nulls.index.tolist()\n",
    "\n",
    "# Fill nulls in these categorical columns\n",
    "for col in categorical_columns_with_nulls_list:\n",
    "    if isinstance(df_tmp3[col].dtype, pd.CategoricalDtype):\n",
    "        df_tmp3[col] = df_tmp3[col].cat.add_categories([\"Unknown\"])\n",
    "    df_tmp3[col] = df_tmp3[col].fillna(\"Unknown\")\n",
    "\n",
    "# Fill nulls for MachineHoursCurrentMeter using its median\n",
    "df_tmp3['MachineHoursCurrentMeter'] = df_tmp3['MachineHoursCurrentMeter'].fillna(\n",
    "    df_tmp3['MachineHoursCurrentMeter'].median()\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# adding farm data\n",
    "# ========================\n",
    "\n",
    "# Ensure state names match by converting to lowercase\n",
    "df_tmp3['state'] = df_tmp3['state'].str.lower()\n",
    "farm_data['State'] = farm_data['State'].str.lower()\n",
    "\n",
    "# Merge farm_data into df_tmp3 based on state and year\n",
    "df_tmp3 = df_tmp3.merge(\n",
    "    farm_data[['State', 'Year', 'Farms_per_Capita', 'Farms_per_Square_Mile']],\n",
    "    left_on=['state', 'saleYear'],\n",
    "    right_on=['State', 'Year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop redundant merge columns (State, Year from farm_data)\n",
    "df_tmp3.drop(columns=['State', 'Year'], inplace=True)\n",
    "\n",
    "# ========================\n",
    "# fixing columns types\n",
    "# ========================\n",
    "\n",
    "# ðŸ”¹ FIX: Convert 'state' back to categorical\n",
    "df_tmp3['state'] = df_tmp3['state'].astype('category')\n",
    "\n",
    "# ========================\n",
    "# size_category column\n",
    "# ========================\n",
    "\n",
    "# Define a mapping for three levels + Unknown\n",
    "size_category_mapping = {\n",
    "    'Compact': 0, 'Mini': 0,  # Small\n",
    "    'Small': 1, 'Medium': 1,  # Medium\n",
    "    'Large': 2, 'Large / Medium': 2,  # Large\n",
    "    'Unknown': -1  # Keep unknown separate\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "df_tmp3['SizeCategory'] = df_tmp3['ProductSize'].map(size_category_mapping)\n",
    "\n",
    "# ========================\n",
    "# Prepare rare caegories lists\n",
    "# ========================\n",
    "\n",
    "# Define how many rare categories you want to extract per column\n",
    "rare_counts = {\n",
    "    'fiProductClassDesc': 34,\n",
    "    'state': 27,\n",
    "    'Undercarriage_Pad_Width': 11,\n",
    "    'Stick_Length': 20\n",
    "}\n",
    "\n",
    "# Dictionary to store the rare categories for each column\n",
    "rare_categories = {}\n",
    "\n",
    "# Iterate over each column and number of rare categories\n",
    "for col, n in rare_counts.items():\n",
    "    # Count the occurrences of each unique value in the column,\n",
    "    # sort them in ascending order, and take the lowest n values.\n",
    "    rare_list = df_tmp3[col].value_counts().sort_values().head(n).index.tolist()\n",
    "    rare_categories[col] = rare_list\n",
    "\n",
    "# ========================\n",
    "# Grouping rare categories\n",
    "# ========================\n",
    "\n",
    "for col, rare_list in rare_categories.items():\n",
    "    # Check if the column is categorical\n",
    "    if pd.api.types.is_categorical_dtype(df_tmp3[col]):\n",
    "        # Add the \"Other\" category if it's not already present\n",
    "        if \"Other\" not in df_tmp3[col].cat.categories:\n",
    "            df_tmp3[col] = df_tmp3[col].cat.add_categories([\"Other\"])\n",
    "    # Replace the values that are in the rare_list with \"Other\"\n",
    "    df_tmp3.loc[df_tmp3[col].isin(rare_list), col] = \"Other\"\n",
    "\n",
    "# ============================\n",
    "# Apply Target Encoding (new)\n",
    "# ============================\n",
    "# Compute target mean encoding for fiSecondaryDesc in training data\n",
    "if 'fiSecondaryDesc' in df_tmp3.columns:\n",
    "    fiSecondaryDesc_target_map = df_tmp3.groupby('fiSecondaryDesc')['SalePrice'].mean()\n",
    "\n",
    "    # Replace fiSecondaryDesc with the mean SalePrice of that category\n",
    "    df_tmp3['fiSecondaryDesc'] = df_tmp3['fiSecondaryDesc'].map(fiSecondaryDesc_target_map)\n",
    "\n",
    "# Compute target mean encoding for fiBaseModel in training data\n",
    "if 'fiBaseModel' in df_tmp3.columns:\n",
    "    fiBaseModel_target_map = df_tmp3.groupby('fiBaseModel')['SalePrice'].mean()\n",
    "\n",
    "    # Replace fiBaseModel with the mean SalePrice of that category\n",
    "    df_tmp3['fiBaseModel'] = df_tmp3['fiBaseModel'].map(fiBaseModel_target_map)\n",
    "\n",
    "# Compute target mean encoding for fiModelDesc in training data\n",
    "if 'fiModelDesc' in df_tmp3.columns:\n",
    "    fiModelDesc_target_map = df_tmp3.groupby('fiModelDesc')['SalePrice'].mean()\n",
    "\n",
    "    # Replace fiBaseModel with the mean SalePrice of that category\n",
    "    df_tmp3['fiModelDesc'] = df_tmp3['fiModelDesc'].map(fiModelDesc_target_map)\n",
    "\n",
    "# Compute target mean encoding for fiProductClassDesc in training data\n",
    "if 'fiProductClassDesc' in df_tmp3.columns:\n",
    "    fiProductClassDesc_target_map = df_tmp3.groupby('fiProductClassDesc')['SalePrice'].mean()\n",
    "\n",
    "    # Replace fiProductClassDesc with the mean SalePrice of that category\n",
    "    df_tmp3['fiProductClassDesc'] = df_tmp3['fiProductClassDesc'].map(fiProductClassDesc_target_map)\n",
    "\n",
    "# ============================\n",
    "# Coding categorical columns\n",
    "# ============================\n",
    "\n",
    "# Convert categorical columns to numeric codes\n",
    "for col in df_tmp3.select_dtypes(['category']):\n",
    "    df_tmp3[col] = df_tmp3[col].cat.codes\n",
    "\n",
    "# ========================\n",
    "# Adjust Prices Using CPI\n",
    "# ========================\n",
    "\n",
    "cpi_data = {\n",
    "    1989: 124.0, 1990: 130.7, 1991: 136.2, 1992: 140.3, 1993: 144.5,\n",
    "    1994: 148.2, 1995: 152.4, 1996: 156.9, 1997: 160.5, 1998: 163.0,\n",
    "    1999: 166.6, 2000: 172.2, 2001: 177.1, 2002: 179.9, 2003: 184.0,\n",
    "    2004: 188.9, 2005: 195.3, 2006: 201.6, 2007: 207.3, 2008: 215.3,\n",
    "    2009: 214.5, 2010: 218.1, 2011: 224.9, 2012: 229.6\n",
    "}\n",
    "\n",
    "# Merge CPI values based on the sale year\n",
    "df_tmp3[\"CPI\"] = df_tmp3[\"saleYear\"].map(cpi_data)\n",
    "\n",
    "# Adjust SalePrice to 2012 equivalent\n",
    "df_tmp3[\"SalePrice\"] = df_tmp3[\"SalePrice\"] * (cpi_data[2012] / df_tmp3[\"CPI\"])\n",
    "\n",
    "# Drop the CPI column if no longer needed\n",
    "df_tmp3.drop(\"CPI\", axis=1, inplace=True)\n",
    "\n",
    "# ========================\n",
    "# Create ValidYearMade Feature\n",
    "# ========================\n",
    "\n",
    "df_tmp3[\"ValidYearMade\"] = ((df_tmp3[\"YearMade\"] != 1000) & (df_tmp3[\"saleYear\"] >= df_tmp3[\"YearMade\"])).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# ========================\n",
    "# ========================\n",
    "# Valid preparation\n",
    "# ========================\n",
    "# ========================\n",
    "\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Sort saledate and copy data\n",
    "# ========================\n",
    "\n",
    "# Sort dataset by date\n",
    "df_valid.sort_values(by=[\"saledate\"], inplace=True, ascending=True)\n",
    "\n",
    "# Copy dataset for preprocessing\n",
    "df_tmp_valid3 = df_valid.copy()\n",
    "\n",
    "# ========================\n",
    "# Extract Date Features\n",
    "# ========================\n",
    "df_tmp_valid3[\"saleYear\"] = df_tmp_valid3.saledate.dt.year\n",
    "df_tmp_valid3[\"saleMonth\"] = df_tmp_valid3.saledate.dt.month\n",
    "df_tmp_valid3[\"saleDay\"] = df_tmp_valid3.saledate.dt.day\n",
    "df_tmp_valid3[\"saleDayofweek\"] = df_tmp_valid3.saledate.dt.dayofweek\n",
    "df_tmp_valid3[\"saleDayofyear\"] = df_tmp_valid3.saledate.dt.dayofyear\n",
    "\n",
    "# Drop original saledate column\n",
    "df_tmp_valid3.drop(\"saledate\", axis=1, inplace=True)\n",
    "\n",
    "# ========================\n",
    "# Transform object columns to categories\n",
    "# ========================\n",
    "\n",
    "# Transform object columns to category\n",
    "for col in df_tmp_valid3.select_dtypes(['object']):\n",
    "    df_tmp_valid3[col] = df_tmp_valid3[col].astype('category')\n",
    "\n",
    "# ========================\n",
    "# filling nulls\n",
    "# ========================\n",
    "\n",
    "# Filling nulls in columns_with_none_or_unspecified\n",
    "for col in columns_with_none_or_unspecified:\n",
    "    if isinstance(df_tmp_valid3[col].dtype, pd.CategoricalDtype):\n",
    "        # Only add category if it's not already present\n",
    "        if \"None or Unspecified\" not in df_tmp_valid3[col].cat.categories:\n",
    "            df_tmp_valid3[col] = df_tmp_valid3[col].cat.add_categories([\"None or Unspecified\"])\n",
    "    df_tmp_valid3[col] = df_tmp_valid3[col].fillna(\"None or Unspecified\")\n",
    "\n",
    "# Filling nulls in categorical_columns_with_nulls\n",
    "for col in categorical_columns_with_nulls_list:\n",
    "    if isinstance(df_tmp_valid3[col].dtype, pd.CategoricalDtype):\n",
    "        df_tmp_valid3[col] = df_tmp_valid3[col].cat.add_categories([\"Unknown\"])\n",
    "    df_tmp_valid3[col] = df_tmp_valid3[col].fillna(\"Unknown\")\n",
    "\n",
    "# Filling MachineHoursCurrentMeter nulls\n",
    "df_tmp_valid3['MachineHoursCurrentMeter'] = df_tmp_valid3['MachineHoursCurrentMeter'].fillna(\n",
    "    df_tmp_valid3['MachineHoursCurrentMeter'].median()\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# adding farm data\n",
    "# ========================\n",
    "\n",
    "# Ensure state names match by converting to lowercase\n",
    "df_tmp_valid3['state'] = df_tmp_valid3['state'].str.lower()\n",
    "farm_data['State'] = farm_data['State'].str.lower()\n",
    "\n",
    "# Merge farm_data into df_tmp3 based on state and year\n",
    "df_tmp_valid3 = df_tmp_valid3.merge(\n",
    "    farm_data[['State', 'Year', 'Farms_per_Capita', 'Farms_per_Square_Mile']],\n",
    "    left_on=['state', 'saleYear'],\n",
    "    right_on=['State', 'Year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop redundant merge columns (State, Year from farm_data)\n",
    "df_tmp_valid3.drop(columns=['State', 'Year'], inplace=True)\n",
    "\n",
    "# ========================\n",
    "# fixing columns types\n",
    "# ========================\n",
    "\n",
    "# ðŸ”¹ FIX: Convert 'state' back to categorical\n",
    "df_tmp_valid3['state'] = df_tmp_valid3['state'].astype('category')\n",
    "\n",
    "# ========================\n",
    "# SizeCategory column\n",
    "# ========================\n",
    "\n",
    "# Create SizeCategory column\n",
    "df_tmp_valid3['SizeCategory'] = df_tmp_valid3['ProductSize'].map(size_category_mapping)\n",
    "\n",
    "# ========================\n",
    "# Grouping rare categories\n",
    "# ========================\n",
    "\n",
    "# Replace rare categories with \"Other\" (using your rare_categories dictionary)\n",
    "for col, rare_list in rare_categories.items():\n",
    "    # Check if the column is categorical\n",
    "    if pd.api.types.is_categorical_dtype(df_tmp_valid3[col]):\n",
    "        # Add the \"Other\" category if it's not already present\n",
    "        if \"Other\" not in df_tmp_valid3[col].cat.categories:\n",
    "            df_tmp_valid3[col] = df_tmp_valid3[col].cat.add_categories([\"Other\"])\n",
    "    # Replace the values that are in the rare_list with \"Other\"\n",
    "    df_tmp_valid3.loc[df_tmp_valid3[col].isin(rare_list), col] = \"Other\"\n",
    "\n",
    "# ============================\n",
    "# Apply Target Encoding (new)\n",
    "# ============================\n",
    "\n",
    "# Apply the trained target encoding to validation data\n",
    "if 'fiSecondaryDesc' in df_tmp_valid3.columns:\n",
    "    # Fill missing categories with the overall mean SalePrice from training data\n",
    "    overall_mean_price = df_tmp3['SalePrice'].mean()\n",
    "\n",
    "    df_tmp_valid3['fiSecondaryDesc'] = df_tmp_valid3['fiSecondaryDesc'].map(fiSecondaryDesc_target_map)\n",
    "\n",
    "    # Handle unseen categories in validation set by filling with overall mean SalePrice\n",
    "    df_tmp_valid3['fiSecondaryDesc'].fillna(overall_mean_price, inplace=True)\n",
    "\n",
    "# Apply the trained target encoding to validation data\n",
    "if 'fiBaseModel' in df_tmp_valid3.columns:\n",
    "    # Fill missing categories with the overall mean SalePrice from training data\n",
    "    overall_mean_price = df_tmp3['SalePrice'].mean()\n",
    "\n",
    "    df_tmp_valid3['fiBaseModel'] = df_tmp_valid3['fiBaseModel'].map(fiBaseModel_target_map)\n",
    "\n",
    "    # Handle unseen categories in validation set by filling with overall mean SalePrice\n",
    "    df_tmp_valid3['fiBaseModel'].fillna(overall_mean_price, inplace=True)\n",
    "\n",
    "# Apply the trained target encoding to validation data\n",
    "if 'fiModelDesc' in df_tmp_valid3.columns:\n",
    "    # Fill missing categories with the overall mean SalePrice from training data\n",
    "    overall_mean_price = df_tmp3['SalePrice'].mean()\n",
    "\n",
    "    df_tmp_valid3['fiModelDesc'] = df_tmp_valid3['fiModelDesc'].map(fiModelDesc_target_map)\n",
    "\n",
    "    # Handle unseen categories in validation set by filling with overall mean SalePrice\n",
    "    df_tmp_valid3['fiModelDesc'].fillna(overall_mean_price, inplace=True)\n",
    "\n",
    "# Apply the trained target encoding to validation data\n",
    "if 'fiProductClassDesc' in df_tmp_valid3.columns:\n",
    "    # Fill missing categories with the overall mean SalePrice from training data\n",
    "    overall_mean_price = df_tmp3['SalePrice'].mean()\n",
    "\n",
    "    df_tmp_valid3['fiProductClassDesc'] = df_tmp_valid3['fiProductClassDesc'].map(fiProductClassDesc_target_map)\n",
    "\n",
    "    # Handle unseen categories in validation set by filling with overall mean SalePrice\n",
    "    df_tmp_valid3['fiProductClassDesc'].fillna(overall_mean_price, inplace=True)\n",
    "\n",
    "# ============================\n",
    "# Coding categorical columns\n",
    "# ============================\n",
    "\n",
    "# Convert all categorical columns in df_tmp_valid3 to numeric codes\n",
    "for col in df_tmp_valid3.select_dtypes(['category']):\n",
    "    df_tmp_valid3[col] = df_tmp_valid3[col].cat.codes\n",
    "\n",
    "# ========================\n",
    "# Create ValidYearMade Feature\n",
    "# ========================\n",
    "\n",
    "# Create ValidYearMade column\n",
    "df_tmp_valid3[\"ValidYearMade\"] = ((df_tmp_valid3[\"YearMade\"] != 1000) &\n",
    "                                  (df_tmp_valid3[\"saleYear\"] >= df_tmp_valid3[\"YearMade\"])).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182be30-a31f-4123-9d3a-8256b6e79e3d",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fc07a-0bbc-4045-a62d-38a0d732361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model train\n",
    "X_train, X_test, y_train, y_test = prepare_data(df_tmp3, 'SalePrice', train_size=0.8)\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_features=0.5,\n",
    "    random_state = 42,\n",
    "    min_samples_leaf=1,\n",
    "    n_jobs=1,\n",
    "    min_samples_split=14,\n",
    "    max_samples=None,\n",
    "    max_depth=25\n",
    ")\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "train_rmse = RMSE(y_train, y_train_pred)\n",
    "test_rmse = RMSE(y_test, y_test_pred)\n",
    "\n",
    "print(f\"train: {train_rmse}\")\n",
    "print(f\"test: {test_rmse}\")\n",
    "\n",
    "df_tmp_valid3['SalePrice'] = model.predict(df_tmp_valid3)\n",
    "\n",
    "new_data = df_tmp_valid3[['SalesID', 'SalePrice']]\n",
    "\n",
    "new_data.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
